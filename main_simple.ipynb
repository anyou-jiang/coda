{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeab9d1-d35c-478c-9d00-c7ad5a375950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a7433-6881-440c-86d6-967c02294add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Beta\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModelSelector:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_next_item_to_label(self):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            (index, selection probability)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def add_label(self, chosen_idx, true_class, selection_prob):\n",
    "        pass\n",
    "\n",
    "    def get_best_model_prediction(self):\n",
    "        pass\n",
    "\n",
    "class Ensemble:\n",
    "    def __init__(self, preds, **kwargs):\n",
    "        self.preds = preds\n",
    "        self.device = preds.device\n",
    "        H, N, C = preds.shape\n",
    "\n",
    "    def get_preds(self, **kwargs):\n",
    "        return self.preds.mean(dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4532dd1-4e16-4c5e-bda4-7680526ee8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_loss(preds, labels, **kwargs):\n",
    "    \"\"\"Get 1 - accuracy (a loss), nonreduced. Handles whether we are working with scores or integer labels.\"\"\"\n",
    "    if len(labels.shape) > 1:\n",
    "        argmaxed_preds = torch.argmax(preds, dim=-1)\n",
    "        argmaxed_labels = torch.argmax(labels, dim=-1)\n",
    "        accs = (argmaxed_preds == argmaxed_labels).float()\n",
    "    else:\n",
    "        argmaxed = torch.argmax(preds, dim=-1)\n",
    "        accs = (argmaxed == labels).float()\n",
    "\n",
    "    # make it a loss\n",
    "    return 1 - accs\n",
    "\n",
    "LOSS_FNS = {\n",
    "    # 'ce': cross_entropy, # TODO this won't work out of the box; we don't have logits\n",
    "    'acc': accuracy_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb3a384-58af-4190-b20e-00aeba7ea2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    A model selection dataset is a tensor of shape (H,N,C) containing post-softmax prediction scores, \n",
    "    where H is the number of models, N is the number of datapoints, and C is the number of classes.\n",
    "\n",
    "    Optionally, it can also contain an (N,) shaped matrix (assumed to be a file appended with '_labels.pt')\n",
    "    of ground-truth class labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath, device):\n",
    "        self.device = device\n",
    "        self.preds = torch.load(filepath, map_location=device).float() # avoid fp16 precision errors\n",
    "        print(\"Loaded preds of shape\", self.preds.shape)\n",
    "\n",
    "        self.labels = None\n",
    "        label_p = filepath.replace('.pt', '_labels.pt')\n",
    "        if os.path.exists(label_p):\n",
    "            self.labels = torch.load(label_p, map_location=device)\n",
    "            print(\"Loaded labels of shape\", self.labels.shape)\n",
    "        else:\n",
    "            print(\"Did not load labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae126e17-a57a-438e-965c-8dff9e1f99b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oracle:\n",
    "    def __init__(self, dataset, loss_fn=None):\n",
    "        self.dataset = dataset\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = dataset.device\n",
    "        self.labels = dataset.labels\n",
    "        assert self.labels is not None, \"Oracle needs labels!\"\n",
    "\n",
    "    def true_losses(self, preds):\n",
    "        \"\"\"\n",
    "        Compute the mean loss for each model.\n",
    "        \n",
    "        Args:\n",
    "        - preds: Tensor of shape (H, N, C) representing post-softmax scores from each model for each data point.\n",
    "        \n",
    "        Returns:\n",
    "        - Tensor of shape (H,) representing the mean loss for each model.\n",
    "        \"\"\"\n",
    "        H, N, C = preds.shape\n",
    "        return self.loss_fn(preds.reshape(-1, C), self.labels.repeat(H), \n",
    "                            reduction='none').view(H, N).mean(dim=1)\n",
    "\n",
    "    def __call__(self, idx):\n",
    "        return self.labels[idx].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962194b3-abe4-425a-93b0-f7f7cface9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_to_beta(alpha_dirichlet: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Get parameters for beta distributions representing the diagonal.\n",
    "    Args:\n",
    "        alpha_dirichlet: shape (..., H, C, C)\n",
    "    Returns:\n",
    "        alpha_cc, beta_cc: shape (..., H, C)\n",
    "    \"\"\"\n",
    "    C = alpha_dirichlet.shape[-1]\n",
    "    alpha_cc = alpha_dirichlet[..., torch.arange(C), torch.arange(C)]\n",
    "    beta_cc  = alpha_dirichlet.sum(dim=-1) - alpha_cc\n",
    "    return alpha_cc, beta_cc\n",
    "\n",
    "\n",
    "def create_confusion_matrices(true_labels: torch.Tensor,\n",
    "                              model_predictions: torch.Tensor,\n",
    "                              mode='hard') -> torch.Tensor:\n",
    "    H, N, C = model_predictions.shape\n",
    "    dev = model_predictions.device\n",
    "    true_one_hot = F.one_hot(true_labels, C).float().to(dev)\n",
    "\n",
    "    if mode == 'hard':\n",
    "        preds = F.one_hot(model_predictions.argmax(-1), C).float()\n",
    "    elif mode == 'soft':\n",
    "        preds = model_predictions\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "\n",
    "    conf = torch.einsum('nc, hnj -> hcj', true_one_hot, preds)\n",
    "    return conf / conf.sum(-1, keepdim=True).clamp_min(1e-6)\n",
    "\n",
    "\n",
    "def initialize_dirichlets(soft_confusion: torch.Tensor,\n",
    "                          prior_strength: float,\n",
    "                          disable_diag_prior=False) -> torch.Tensor:\n",
    "    H, C, _ = soft_confusion.shape\n",
    "\n",
    "    if disable_diag_prior:\n",
    "        # uniform - 2 pseudo counts per row to match diag method\n",
    "        base = torch.full((C, C), 2 / C,\n",
    "                          dtype=soft_confusion.dtype,\n",
    "                          device=soft_confusion.device)\n",
    "    else:\n",
    "        base = torch.full((C, C), 1.0 / (C - 1),\n",
    "                            dtype=soft_confusion.dtype,\n",
    "                            device=soft_confusion.device)\n",
    "        base.fill_diagonal_(1.0)\n",
    "\n",
    "    base = base.unsqueeze(0).expand(H, C, C)\n",
    "    return base + prior_strength * soft_confusion\n",
    "\n",
    "\n",
    "def batch_update_dirichlet_for_item(dirichlet_alphas: torch.Tensor,\n",
    "                                    classifier_preds: torch.Tensor,\n",
    "                                    update_weight: float = 1.0) -> torch.Tensor:\n",
    "    N, H, C = classifier_preds.shape\n",
    "    updated = dirichlet_alphas[None, None].expand(N, C, H, C, C).clone()\n",
    "    updates = classifier_preds[:, None].expand(-1, C, -1, -1) * update_weight\n",
    "    for c in range(C):\n",
    "        updated[:, c, :, c, :] += updates[:, c, :, :]\n",
    "    return updated\n",
    "\n",
    "\n",
    "def compute_pbest_beta_batched(alpha_batch: torch.Tensor,  # (B_, C_, C, H)\n",
    "                                beta_batch:  torch.Tensor, # (B_, C_, C, H)\n",
    "                                num_points: int = 256,\n",
    "                                eps: float = 1e-30,\n",
    "                                chunk_size: int = None) -> torch.Tensor:\n",
    "    device = alpha_batch.device\n",
    "    N = alpha_batch.shape[0]\n",
    "    C, H = alpha_batch.shape[-2:]\n",
    "    chunk_size = chunk_size or N\n",
    "    x = torch.linspace(1e-6, 1 - 1e-6, num_points, device=device).unsqueeze(-1) # P×1\n",
    "\n",
    "    prob_out = torch.zeros_like(alpha_batch)\n",
    "    for start in range(0, N, chunk_size):\n",
    "        end = min(start + chunk_size, N)\n",
    "        a_flat = alpha_batch[start:end].reshape(-1, H)          # B_*C_*C × H\n",
    "        b_flat = beta_batch[start:end].reshape(-1, H)\n",
    "        \n",
    "        logpdf = Beta(a_flat.reshape(-1), b_flat.reshape(-1)).log_prob(x)\n",
    "        pdf = logpdf.exp().T.reshape(-1, H, num_points)         # B_*C_*C × H × P\n",
    "\n",
    "        cdf = torch.zeros_like(pdf)\n",
    "        for j in range(1, num_points):\n",
    "            dx = x[j] - x[j-1]\n",
    "            cdf[:, :, j] = cdf[:, :, j-1] + 0.5*(pdf[:, :, j] + pdf[:, :, j-1])*dx\n",
    "\n",
    "        log_cdf = torch.log(cdf.clamp_min(eps))\n",
    "        # clamp to min/max float32 +-(log(3.4 * 1e38) = ~88) to avoid inf; \n",
    "        # rare that this happens (only observed with uniform prior)\n",
    "        prod_excl = torch.exp( (log_cdf.sum(1, keepdim=True) - log_cdf).clamp(-80,80) ) \n",
    "        integrand = pdf * prod_excl\n",
    "\n",
    "        prob = torch.trapz(integrand, x.squeeze(), dim=2)\n",
    "        prob = prob / prob.sum(-1, keepdim=True).clamp_min(eps)\n",
    "        prob_out[start:end] = prob.reshape(alpha_batch[start:end].shape)\n",
    "\n",
    "    return prob_out # (B_, C_, C, H)\n",
    "\n",
    "\n",
    "def pbest_row_mixture_batched(updated_dirichlet: torch.Tensor,\n",
    "                                pi_hat: torch.Tensor,\n",
    "                                num_points: int = 256) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        updated_dirichlet: (B_, C_, H, C, C)\n",
    "        pi: (C,)\n",
    "    where:\n",
    "        B_ and C_ are additional dimensions that can be used for hypothetical item and hypothetical class updates, respectively\n",
    "            (i.e. all operations are broadcast over B_ and C_)\n",
    "        pi is the marginal class distribution P(class=C) over the entire dataset\n",
    "    \n",
    "    Returns:\n",
    "        prob_best: (B_, C_, H),  P(h is best | C_, B_)\n",
    "    \"\"\"\n",
    "    C = updated_dirichlet.shape[-1]\n",
    "\n",
    "    # P(h is best | row c)\n",
    "    alpha_cc, beta_cc = dirichlet_to_beta(updated_dirichlet)\n",
    "    prob_best_b_c_ch = compute_pbest_beta_batched(alpha_cc.transpose(-1, -2), beta_cc.transpose(-1, -2), num_points=num_points) # (B_,C_,C,H)\n",
    "\n",
    "    # convert conditional to marginal probabilities using pi_hat\n",
    "    # expected P(best | item b) = Σ_c expected P(best | item b, class=c) * P(class=c)\n",
    "    marginal_probs = (prob_best_b_c_ch * pi_hat.view(1, C, 1)).sum(-2)  # (B_, C_, H)\n",
    "\n",
    "    return marginal_probs\n",
    "\n",
    "\n",
    "def batch_update_beta(selector, # selector.dirichlets: (H,C,C)\n",
    "                      preds,    # (B, H)\n",
    "                      update_weight=1.0\n",
    "                      ): \n",
    "    B, H = preds.shape\n",
    "    C = selector.dirichlets.shape[-1]\n",
    "    alpha_cc_before, beta_cc_before = dirichlet_to_beta(selector.dirichlets) # (H, C)\n",
    "\n",
    "    pred_classes = preds.unsqueeze(1).expand(B,C,H)\n",
    "    class_range  = torch.arange(C, device=alpha_cc_before.device).unsqueeze(1).expand(B,C,H)\n",
    "    eq_mask = (pred_classes == class_range) # B,C,H\n",
    "    eq_mask = eq_mask.permute(0,2,1) # B,H,C\n",
    "\n",
    "    alpha_batch = alpha_cc_before.expand(B, H, C).clone()\n",
    "    beta_batch = beta_cc_before.expand(B, H, C).clone()\n",
    "    alpha_batch[eq_mask] += 1.0 * update_weight\n",
    "    beta_batch[~eq_mask]  += 1.0 * update_weight\n",
    "\n",
    "    return alpha_batch, beta_batch # (B, H, C), (B, H, C)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8c2ec-8890-4a48-ac92-d5c759b00976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CODA(ModelSelector):\n",
    "    def __init__(self, \n",
    "                 dataset,\n",
    "                 prefilter_n=0,\n",
    "                 alpha=0.9,\n",
    "                 learning_rate=0.01,\n",
    "                 multiplier=2.0,\n",
    "                 disable_diag_prior=False,  # for ablation 1\n",
    "                 q='eig',                   # for ablation 2\n",
    "                 ):\n",
    "        self.dataset = dataset\n",
    "        self.device = dataset.preds.device\n",
    "        self.H, self.N, self.C = dataset.preds.shape\n",
    "        self.prefilter_n = prefilter_n\n",
    "        self.disable_diag_prior = disable_diag_prior\n",
    "        self.q = q\n",
    "\n",
    "        # hyperparams\n",
    "        self.prior_strength = (1 - alpha)\n",
    "        self.update_strength = learning_rate\n",
    "\n",
    "        # initialize dirichlets\n",
    "        ens_pred = Ensemble(dataset.preds).get_preds()\n",
    "        ens_pred_hard = ens_pred.argmax(-1)  # pseudo labels\n",
    "        soft_conf = create_confusion_matrices(ens_pred_hard, dataset.preds, mode='soft')\n",
    "        self.dirichlets = multiplier * initialize_dirichlets(soft_conf, self.prior_strength, self.disable_diag_prior)\n",
    "        self.update_pi_hat()\n",
    "\n",
    "        self.labeled_idxs, self.labels = [], []\n",
    "        self.unlabeled_idxs = list(range(self.N))\n",
    "        self.q_vals = []\n",
    "        self.stochastic = False\n",
    "        self.step = 0\n",
    "\n",
    "    @classmethod\n",
    "    def from_args(cls, dataset, args):\n",
    "        return cls(dataset,\n",
    "                   prefilter_n=args.prefilter_n,\n",
    "                   alpha=args.alpha,\n",
    "                   learning_rate=args.learning_rate,\n",
    "                   multiplier=args.multiplier,\n",
    "                   disable_diag_prior=args.no_diag_prior,\n",
    "                   q=args.q)\n",
    "\n",
    "    def _prefilter(self, idxs):\n",
    "        # filter any data points where every model disagrees - waste of compute\n",
    "        maj, _ = torch.mode(self.dataset.preds.argmax(-1), dim=0)\n",
    "        mask = (self.dataset.preds.argmax(-1) != maj).sum(0) > 0\n",
    "        idxs = [i for i in idxs if mask[i]]\n",
    "        # can also randomly subsample (disabled by default)\n",
    "        if self.prefilter_n and len(idxs) > self.prefilter_n:\n",
    "            idxs = random.sample(idxs, self.prefilter_n)\n",
    "            self.stochastic = True\n",
    "        return idxs\n",
    "\n",
    "    def update_pi_hat(self):\n",
    "        adjusted = torch.einsum('hcs, hns -> hnc', self.dirichlets, self.dataset.preds)\n",
    "        # per item\n",
    "        self.pi_hat_xi = adjusted.sum(0)\n",
    "        self.pi_hat_xi = self.pi_hat_xi / self.pi_hat_xi.sum(dim=-1, keepdim=True).clamp_(min=1e-12)\n",
    "        # marginal (entire dataset)\n",
    "        self.pi_hat = self.pi_hat_xi.sum(0)\n",
    "        self.pi_hat = self.pi_hat / self.pi_hat.sum()\n",
    "\n",
    "    def eig_batched(self, chunk_size: int = 100, update_weight: float = 1.0, num_points: int = 256):\n",
    "        \"\"\"\n",
    "            TODO: Document shapes etc.\n",
    "        \"\"\"\n",
    "        candidate_ids = self._prefilter(self.unlabeled_idxs) or self.unlabeled_idxs\n",
    "        classifier_preds = self.dataset.preds.permute(1, 0, 2)\n",
    "        candidates = torch.tensor(candidate_ids, device=classifier_preds.device)\n",
    "        N, H, C = classifier_preds.shape\n",
    "\n",
    "        # compute current pbest per row\n",
    "        dirichlets_before = self.dirichlets.unsqueeze(0).unsqueeze(0).expand(1, 1, H, C, C)\n",
    "        \n",
    "        # get diagonal betas\n",
    "        alpha_cc_before, beta_cc_before = dirichlet_to_beta(dirichlets_before) # (1, 1, H, C)\n",
    "        alpha_cc_before = alpha_cc_before.permute(0,3,1,2)  # (1, C, 1, H)\n",
    "        beta_cc_before  = beta_cc_before.permute(0,3,1,2)   # (1, C, 1, H)\n",
    "        pbest_rows_before = compute_pbest_beta_batched(alpha_cc_before, beta_cc_before).squeeze(-2) # (1, C, H)\n",
    "\n",
    "        mixture0 = (self.pi_hat[:, None] * pbest_rows_before).sum(1)   # (1,H)\n",
    "        H_before = -(mixture0.clamp_min(1e-12).mul(mixture0.clamp_min(1e-12).log2())).sum(-1)\n",
    "\n",
    "        # broadcast helpers\n",
    "        mixture0_bc = mixture0.view(1, 1, H)      # (1,1,H)\n",
    "        pi_hat_row  = self.pi_hat.view(1, C, 1)   # (1,C,1)\n",
    "\n",
    "        eig_chunks = []\n",
    "        for s in tqdm(range(0, len(candidates), chunk_size)):\n",
    "            ids   = candidates[s:s + chunk_size] # (B,)\n",
    "            preds = classifier_preds[ids].argmax(-1) # (B, H)\n",
    "            pi_hat_xi = self.pi_hat_xi[ids]\n",
    "\n",
    "            # do all hypothetical updates at once\n",
    "            alpha_hypothetical, beta_hypothetical = batch_update_beta(self, preds, update_weight) # (B,H,C_)\n",
    "            alpha_hypothetical = alpha_hypothetical.permute(0,2,1).unsqueeze(-2)  # (B, C_, 1, H)\n",
    "            beta_hypothetical = beta_hypothetical.permute(0,2,1).unsqueeze(-2)    # (B, C_, 1, H)\n",
    "\n",
    "            pbest_hypothetical_rows = compute_pbest_beta_batched(alpha_hypothetical, \n",
    "                                                                    beta_hypothetical, \n",
    "                                                                    num_points=num_points).squeeze(-2) # (B, C_, H)\n",
    "            deltas = pi_hat_row * (pbest_hypothetical_rows - pbest_rows_before) # (B,C,H)\n",
    "            mix_new = mixture0_bc + deltas # (B,C,H)\n",
    "            H_after = -(mix_new.clamp_min(1e-12).mul(mix_new.clamp_min(1e-12).log2())).sum(-1) # (B,C)\n",
    "            \n",
    "            eig = H_before - (pi_hat_xi * H_after).sum(-1) # (B,)\n",
    "            eig_chunks.append(eig)\n",
    "\n",
    "        return torch.cat(eig_chunks), candidate_ids\n",
    "\n",
    "    def get_next_item_to_label(self):\n",
    "        if self.q == 'eig':\n",
    "            # default; expected information gain\n",
    "            q_vals, cand = self.eig_batched()\n",
    "        else:\n",
    "            raise NotImplementedError(self.q)\n",
    "\n",
    "        # greedy sampling with random selection between ties\n",
    "        best = q_vals.max()\n",
    "        ties = torch.isclose(q_vals, best, rtol=1e-8)\n",
    "        idx_local = random.choice(torch.nonzero(ties, as_tuple=True)[0].tolist()) \\\n",
    "                    if ties.sum() > 1 else torch.argmax(q_vals).item()\n",
    "        if ties.sum() > 1:\n",
    "            self.stochastic = True\n",
    "\n",
    "        return cand[idx_local], q_vals[idx_local].item()\n",
    "\n",
    "    def add_label(self, idx, true_class, selection_prob):\n",
    "        preds = F.one_hot(self.dataset.preds[:, idx].argmax(-1), self.C).float()\n",
    "        self.dirichlets[:, true_class] += self.update_strength * preds\n",
    "\n",
    "        self.update_pi_hat()\n",
    "        self.labeled_idxs.append(idx)\n",
    "        self.labels.append(int(true_class))\n",
    "        self.q_vals.append(selection_prob)\n",
    "        self.unlabeled_idxs.remove(idx)\n",
    "\n",
    "    def get_pbest(self):\n",
    "        H, C, _ = self.dirichlets.shape\n",
    "        expanded = self.dirichlets.unsqueeze(0).unsqueeze(0).expand(1, 1, H, C, C)\n",
    "        pbest = pbest_row_mixture_batched(expanded, self.pi_hat).squeeze(0) # (H,)\n",
    "\n",
    "        return pbest\n",
    "\n",
    "    def get_best_model_prediction(self):\n",
    "        pbest = self.get_pbest()\n",
    "    \n",
    "        # track how many times we've done this\n",
    "        self.step += 1 \n",
    "\n",
    "        return torch.argmax(pbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0f98a-31bb-4e9f-9050-e71310683440",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_DB = False\n",
    "if USE_DB:\n",
    "    mlflow.set_tracking_uri('sqlite:///coda.sqlite')\n",
    "\n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def parse_args(optiions):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # dataset settings\n",
    "    parser.add_argument(\"--task\", help=\"{ 'sketch_painting', ... }\", default=None)\n",
    "    parser.add_argument(\"--data-dir\", default='data')\n",
    "\n",
    "    # benchmarking settings\n",
    "    parser.add_argument(\"--iters\", type=int, default=100)\n",
    "    parser.add_argument(\"--seeds\", type=int, default=5) # how many seeds to use - one experiment per seed\n",
    "    parser.add_argument(\"--force-rerun\", action=\"store_true\", help=\"Overwrite existing runs.\")\n",
    "    parser.add_argument(\"--experiment-name\", default=None) # overrides default of using task as experiment name\n",
    "    parser.add_argument(\"--no-mlflow\", action=\"store_true\", help=\"Disable MLflow logging.\")\n",
    "\n",
    "    # general method settings\n",
    "    parser.add_argument(\"--loss\", help=\"{ 'ce', 'acc', ... }\", default=\"acc\",)\n",
    "    parser.add_argument(\"--method\", help=\"{ 'iid', 'beta', 'activetesting', 'vma' }\", default='iid')\n",
    "    \n",
    "    # CODA settings\n",
    "    parser.add_argument(\"--alpha\", default=0.9, type=float)      # TODO: change to 1-alpha\n",
    "    parser.add_argument(\"--learning-rate\", default=0.01, type=float)\n",
    "    parser.add_argument(\"--multiplier\", default=2.0, type=float) # TODO: change to temperature\n",
    "    parser.add_argument(\"--prefilter-n\", type=int, default=0, help=\"Subsample n test data points each iteration. Useful for speeding up EIG calculations on large datsets. Disabled by default.\")\n",
    "    parser.add_argument(\"--no-diag-prior\", action=\"store_true\", help=\"Disable diagonal prior (Eq 7); used for ablation 1.\")\n",
    "    parser.add_argument(\"--q\", default=\"eig\", help=\"Acquisition function {eig, iid, uncertainty}. Default EIG (eq 17). Used for ablation 2.\")\n",
    "\n",
    "    return parser.parse_args(optiions)\n",
    "\n",
    "def do_model_selection_experiment(dataset, oracle, args, loss_fn, seed=0):\n",
    "    seed_all(seed)\n",
    "    true_losses = oracle.true_losses(dataset.preds)\n",
    "    best_loss = min(oracle.true_losses(dataset.preds))\n",
    "    print(\"Best possible loss is\", best_loss)\n",
    "\n",
    "    # initialize method\n",
    "    if args.method == 'iid':\n",
    "        selector = IID(dataset, loss_fn)\n",
    "    elif args.method == 'uncertainty':\n",
    "        selector = Uncertainty(dataset, loss_fn)\n",
    "    elif args.method.startswith('coda'):\n",
    "        selector = CODA.from_args(dataset, args)\n",
    "    elif args.method == 'activetesting':\n",
    "        selector = ActiveTesting(dataset, loss_fn)\n",
    "    elif args.method == 'vma':\n",
    "        selector = VMA(dataset, loss_fn)\n",
    "    elif args.method == 'model_picker':\n",
    "        from coda.baselines.modelpicker import TASK_EPS\n",
    "        if args.task in TASK_EPS.keys():\n",
    "            selector = ModelPicker(dataset, epsilon=TASK_EPS[args.task])\n",
    "        else:\n",
    "            print(args.task, \"not in TASK_EPS; using default\")\n",
    "            selector = ModelPicker(dataset)\n",
    "    else:\n",
    "        raise ValueError(args.method + \" is not a supported method.\")\n",
    "\n",
    "    # Get prior regret\n",
    "    best_model_idx_pred = selector.get_best_model_prediction()\n",
    "    regret_loss = true_losses[best_model_idx_pred] - best_loss\n",
    "    print(\"Regret at 0:\", regret_loss)\n",
    "\n",
    "    ## Active model selection loop\n",
    "    cumulative_regret_loss = 0\n",
    "    for m in tqdm(range(args.iters)):\n",
    "        # select item, label, select model\n",
    "        chosen_idx, selection_prob = selector.get_next_item_to_label()\n",
    "        true_class = oracle(chosen_idx)\n",
    "        selector.add_label(chosen_idx, true_class, selection_prob)\n",
    "        best_model_idx_pred = selector.get_best_model_prediction()\n",
    "\n",
    "        # compute and log metrics\n",
    "        regret_loss = true_losses[best_model_idx_pred] - best_loss\n",
    "        cumulative_regret_loss += regret_loss\n",
    "        print(\"Regret at\", m+1, \":\", regret_loss)\n",
    "        print(\"Cuml Regret at\", m+1, \":\", cumulative_regret_loss)\n",
    "        if not args.no_mlflow:\n",
    "            mlflow.log_metric(\"regret\", regret_loss.item(), step=m+1)\n",
    "            mlflow.log_metric(\"cumulative regret\", cumulative_regret_loss.item(), step=m+1)\n",
    "\n",
    "    return selector.stochastic\n",
    "\n",
    "def main(options):\n",
    "    args = parse_args(options)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device is\", device)\n",
    "\n",
    "    # Load prediction results of all hypotheses\n",
    "    dataset = Dataset(os.path.join(args.data_dir, args.task + \".pt\"), device=device)\n",
    "\n",
    "    # Create oracle\n",
    "    loss_fn = LOSS_FNS[args.loss]\n",
    "    oracle = Oracle(dataset, loss_fn=loss_fn)\n",
    "    \n",
    "    ## Model selection loop\n",
    "    if args.no_mlflow:\n",
    "        # simple run loop without MLflow logging\n",
    "        for seed in range(args.seeds):\n",
    "            print(\"Running active model selection with seed\", seed)\n",
    "            print(\"DEBUG ARGS\", args.__dict__)\n",
    "            seed_stochastic = do_model_selection_experiment(dataset, oracle, args, loss_fn, seed=seed)\n",
    "\n",
    "            if not seed_stochastic:\n",
    "                print(\"Method is not stochastic for this task. Skipping further seeds.\")\n",
    "                break\n",
    "    else:\n",
    "        # create mlflow 'experiment' (= dataset/task)\n",
    "        experiment_name = args.experiment_name or args.task\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        def get_mlflow_run_id(run_name):\n",
    "            run_id = None\n",
    "            matching_runs = mlflow.search_runs(experiment_names=[experiment_name], filter_string=f\"tags.mlflow.runName = '{run_name}'\", max_results=1)\n",
    "            finished = False\n",
    "            stochastic = None\n",
    "            if len(matching_runs):\n",
    "                run_id = matching_runs.run_id.values[0]\n",
    "                finished = matching_runs.status.values[0] == 'FINISHED'\n",
    "                stochastic = 'params.stochastic' in matching_runs.columns and matching_runs['params.stochastic'].values[0] == 'True'\n",
    "            return run_id, finished, stochastic\n",
    "\n",
    "        # create mlflow 'run' (= algorithm)\n",
    "        run_name = \"-\".join([experiment_name, args.method])\n",
    "        run_id, _, _ = get_mlflow_run_id(run_name)\n",
    "        with mlflow.start_run(run_id=run_id, run_name=run_name):\n",
    "            mlflow.log_params(args.__dict__)\n",
    "            for seed in range(args.seeds):\n",
    "                # create nested ml flow 'run' (= seed)\n",
    "                seed_run_name = \"-\".join([experiment_name, args.method, str(seed)])\n",
    "                seed_run_id, seed_finished, seed_stochastic = get_mlflow_run_id(seed_run_name)\n",
    "                if seed_finished and not args.force_rerun:\n",
    "                    print(\"Seed\", seed, \"finished. Skipping.\")\n",
    "                else:\n",
    "                    with mlflow.start_run(nested=True, run_id=seed_run_id, run_name=seed_run_name):\n",
    "                        mlflow.log_param(\"seed\", seed)\n",
    "                        print(\"Running active model selection with seed\", seed)\n",
    "                        print(\"DEBUG ARGS\", args.__dict__)\n",
    "                        seed_stochastic = do_model_selection_experiment(dataset, oracle, args, loss_fn, seed=seed)\n",
    "                        mlflow.log_param(\"stochastic\", seed_stochastic)\n",
    "\n",
    "                if not seed_stochastic:\n",
    "                    print(\"Method is not stochastic for this task. Skipping further seeds.\")\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aaa554-4b2e-4035-861b-106a480544dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(['--task', 'cifar10_5592', '--method', 'coda', '--data-dir', 'dataset'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
